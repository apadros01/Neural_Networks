{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_facial_point_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apadros01/Neural_Networks/blob/main/CNN_facial_point_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZRSiDYNQLSr"
      },
      "source": [
        "# Aquest notebook té 25GB"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji347a6K-vUL"
      },
      "source": [
        "# Facial Point Detection using Convolutional Neural Networks\n",
        "Author: Àlex Padrós Zamora\n",
        "\n",
        "Github: https://github.com/apadros01\n",
        "\n",
        "LinkedIn: https://www.linkedin.com/in/alexpadroszamora/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx3_KqdfRhWU"
      },
      "source": [
        "The following 3 cells are necessary for performing the operations with GPU computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXnDmXR7RDr2",
        "outputId": "ebba2c9e-d07b-4267-cf24-6aa74e3e0b8c"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqawB1uoBDuI"
      },
      "source": [
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJzNz9ZsBJLR",
        "outputId": "92fde87e-e1eb-4951-8eab-c5e49ff0b7ab"
      },
      "source": [
        "gpu()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=403.0265>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWSpv33WRvpb"
      },
      "source": [
        "The following 3 cells are the ones that download the data. It's just a copy-paste of what was given in the statement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIm7haLg-pn2",
        "outputId": "afdedadd-6122-441b-c841-ef27aec13a2e"
      },
      "source": [
        "# downloading data\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from matplotlib import pyplot\n",
        "import os\n",
        "import time\n",
        "\n",
        "from pandas import DataFrame\n",
        "from pandas.io.parsers import read_csv\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "!wget https://www.dropbox.com/s/svrvpus93131m98/test.csv\n",
        "!wget https://www.dropbox.com/s/xzm5f5vrx6jwqui/training.csv"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-22 09:50:04--  https://www.dropbox.com/s/svrvpus93131m98/test.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6021:18::a27d:4112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/svrvpus93131m98/test.csv [following]\n",
            "--2022-02-22 09:50:04--  https://www.dropbox.com/s/raw/svrvpus93131m98/test.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb1135dc8bc9b0402227ae50a7d.dl.dropboxusercontent.com/cd/0/inline/BgPhuaVupf4X4yn9AnnPTj3kYSyCrh4s1J1ffvniwxFGihPZ4-XKif5AgacGtkj1hUil_wGzqp4PXm2B3xYByPZuz2tAGCMCeJKWBRn9yqTDY5weFOph5NztqFzty0FQCNkqdd_8RknjFaUSLoUrIWYB/file# [following]\n",
            "--2022-02-22 09:50:05--  https://ucb1135dc8bc9b0402227ae50a7d.dl.dropboxusercontent.com/cd/0/inline/BgPhuaVupf4X4yn9AnnPTj3kYSyCrh4s1J1ffvniwxFGihPZ4-XKif5AgacGtkj1hUil_wGzqp4PXm2B3xYByPZuz2tAGCMCeJKWBRn9yqTDY5weFOph5NztqFzty0FQCNkqdd_8RknjFaUSLoUrIWYB/file\n",
            "Resolving ucb1135dc8bc9b0402227ae50a7d.dl.dropboxusercontent.com (ucb1135dc8bc9b0402227ae50a7d.dl.dropboxusercontent.com)... 162.125.66.15, 2620:100:6028:15::a27d:470f\n",
            "Connecting to ucb1135dc8bc9b0402227ae50a7d.dl.dropboxusercontent.com (ucb1135dc8bc9b0402227ae50a7d.dl.dropboxusercontent.com)|162.125.66.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/plain]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "test.csv                [                <=> ]  57.05M  15.3MB/s    in 3.9s    \n",
            "\n",
            "2022-02-22 09:50:09 (14.8 MB/s) - ‘test.csv’ saved [59822141]\n",
            "\n",
            "--2022-02-22 09:50:09--  https://www.dropbox.com/s/xzm5f5vrx6jwqui/training.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6027:18::a27d:4812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/xzm5f5vrx6jwqui/training.csv [following]\n",
            "--2022-02-22 09:50:10--  https://www.dropbox.com/s/raw/xzm5f5vrx6jwqui/training.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucf9fdae12b7686c42e2be5b96c6.dl.dropboxusercontent.com/cd/0/inline/BgNK09kmn44lE2SqPu2Em2XIEti2z4TBMxBAcxBJfAx9lXXWKP2avgPWGQCeqtKxIpWdqUUe-V2TIpARoS9GvXuUD01Nt8drmNS_wIn-UTDZQ6xZKa3-wAFzGYaFkNB42qhef0O077ErxcHmHIUoFlUL/file# [following]\n",
            "--2022-02-22 09:50:10--  https://ucf9fdae12b7686c42e2be5b96c6.dl.dropboxusercontent.com/cd/0/inline/BgNK09kmn44lE2SqPu2Em2XIEti2z4TBMxBAcxBJfAx9lXXWKP2avgPWGQCeqtKxIpWdqUUe-V2TIpARoS9GvXuUD01Nt8drmNS_wIn-UTDZQ6xZKa3-wAFzGYaFkNB42qhef0O077ErxcHmHIUoFlUL/file\n",
            "Resolving ucf9fdae12b7686c42e2be5b96c6.dl.dropboxusercontent.com (ucf9fdae12b7686c42e2be5b96c6.dl.dropboxusercontent.com)... 162.125.71.15, 2620:100:6027:15::a27d:480f\n",
            "Connecting to ucf9fdae12b7686c42e2be5b96c6.dl.dropboxusercontent.com (ucf9fdae12b7686c42e2be5b96c6.dl.dropboxusercontent.com)|162.125.71.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/plain]\n",
            "Saving to: ‘training.csv’\n",
            "\n",
            "training.csv            [ <=>                ] 227.04M  15.4MB/s    in 15s     \n",
            "\n",
            "2022-02-22 09:50:26 (14.9 MB/s) - ‘training.csv’ saved [238064810]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvNuoISw-2oa"
      },
      "source": [
        "# loading data\n",
        "\n",
        "FTRAIN = 'training.csv'\n",
        "FTEST = 'test.csv'\n",
        "FLOOKUP = 'IdLookupTable.csv'\n",
        "\n",
        "def load(test = False, cols = None):\n",
        "    \"\"\"\n",
        "    Loads the dataset.\n",
        "    Returns a tuple of X and y, if `test` was set to `True` y contains `None`.    \n",
        "    \"\"\"\n",
        "    \n",
        "    fname = FTEST if test else FTRAIN\n",
        "    df = read_csv(os.path.expanduser(fname))  # load pandas dataframe\n",
        "\n",
        "    # The Image column has pixel values separated by space; convert\n",
        "    # the values to numpy arrays:\n",
        "    df['Image'] = df['Image'].apply(lambda im: np.fromstring(im, sep = ' '))\n",
        "\n",
        "    if cols:  # get a subset of columns\n",
        "        df = df[list(cols) + ['Image']]\n",
        "    print(\"There are missing points:\")\n",
        "    print(df.count())  # prints the number of values for each column\n",
        "    df = df.dropna()  # drop all rows that have missing values in them\n",
        "\n",
        "    X = np.vstack(df['Image'].values) / 255.  # scale pixel values to [0, 1]\n",
        "    X = X.astype(np.float32)\n",
        "\n",
        "    if not test:  # only FTRAIN has any target columns\n",
        "        y = df[df.columns[:-1]].values\n",
        "        y = (y - 48) / 48  # scale target coordinates to [-1, 1]\n",
        "        X, y = shuffle(X, y, random_state=42)  # shuffle train data\n",
        "        y = y.astype(np.float32)\n",
        "    else:\n",
        "        y = None\n",
        "\n",
        "    return X.reshape(df.shape[0],96,96), y\n",
        "\n",
        "def plot_sample(x, y):\n",
        "    \"\"\"\n",
        "    Plots a single sample image with keypoints on top.   \n",
        "    \"\"\"\n",
        "    pyplot.imshow(x, cmap='gray')\n",
        "    pyplot.scatter(y[0::2] * 48 + 48, y[1::2] * 48 + 48, marker='x', s=10)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "id": "BuV0r84M-7dx",
        "outputId": "ab282d6d-ee8a-406d-d750-4208a334c31b"
      },
      "source": [
        "# loading data and checking it\n",
        "\n",
        "X, y = load()\n",
        "plot_sample(X[12],y[12])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are missing points:\n",
            "left_eye_center_x            7039\n",
            "left_eye_center_y            7039\n",
            "right_eye_center_x           7036\n",
            "right_eye_center_y           7036\n",
            "left_eye_inner_corner_x      2271\n",
            "left_eye_inner_corner_y      2271\n",
            "left_eye_outer_corner_x      2267\n",
            "left_eye_outer_corner_y      2267\n",
            "right_eye_inner_corner_x     2268\n",
            "right_eye_inner_corner_y     2268\n",
            "right_eye_outer_corner_x     2268\n",
            "right_eye_outer_corner_y     2268\n",
            "left_eyebrow_inner_end_x     2270\n",
            "left_eyebrow_inner_end_y     2270\n",
            "left_eyebrow_outer_end_x     2225\n",
            "left_eyebrow_outer_end_y     2225\n",
            "right_eyebrow_inner_end_x    2270\n",
            "right_eyebrow_inner_end_y    2270\n",
            "right_eyebrow_outer_end_x    2236\n",
            "right_eyebrow_outer_end_y    2236\n",
            "nose_tip_x                   7049\n",
            "nose_tip_y                   7049\n",
            "mouth_left_corner_x          2269\n",
            "mouth_left_corner_y          2269\n",
            "mouth_right_corner_x         2270\n",
            "mouth_right_corner_y         2270\n",
            "mouth_center_top_lip_x       2275\n",
            "mouth_center_top_lip_y       2275\n",
            "mouth_center_bottom_lip_x    7016\n",
            "mouth_center_bottom_lip_y    7016\n",
            "Image                        7049\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3uPW0VopbBK"
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfP5ujV1SNXF"
      },
      "source": [
        "The main trick will be the usage of data augmentation.\n",
        "\n",
        "The *add_brightness()* function is used to increase the brightness of images of the x_train. Parameters a and b allow us to perform some experiments, and they must be set in a way that increase the values of x_train. We use the numpy.minimum(1,...) function because we have scaled the pixel values to [0, 1].\n",
        "\n",
        "The *add_darkness* function is used in the opposite way, it is used to darken the images. The parameters a and b must be set in a way that reduce the values of x_train. Since we don't want to get a negative value, we use the numpy.maximum(0,...) function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLPkBk5tqTM2"
      },
      "source": [
        "def add_brightness(x,a,b):\n",
        "  nou = np.minimum(1,a*x+b)\n",
        "  return nou\n",
        "\n",
        "def add_darkness(x,a,b):\n",
        "  nou=np.maximum(0,a*x+b)\n",
        "  return nou\n",
        "\n",
        "from keras import backend as K\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v0pTUHmU8Ph"
      },
      "source": [
        "Related to the convolutional part, we add some 2D convolutional layers, and then a Dropout of 20% of the neurons.\n",
        "\n",
        "In the fully-connected part, we add two dense layers with dimensionality of the output space equal to 300.\n",
        "For both parts, the activation function will be relu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhD2kcDYqX5m"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Input, Conv2D, Lambda\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Predefined parameters\n",
        "input_shape = (96, 96, 1)\n",
        "output_shape = 30\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3),activation='relu', padding=\"same\"),     \n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "\n",
        "        layers.Dense(300,activation='relu'),\n",
        "        layers.Dense(300,activation='relu'),\n",
        "        layers.Dense(output_shape, activation=\"linear\")\n",
        "        \n",
        "    ]\n",
        ")\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiPP1MJNYwBr"
      },
      "source": [
        "With this function, we create the dataset depending on the parameters a,b,c,d and the random_state number:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffgl3_vt_6x4"
      },
      "source": [
        "def dataset(random_state,a,b,c,d):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,random_state=random_state)\n",
        "  x_test, x_valid, y_test, y_valid = train_test_split(x_test, y_test, test_size = 0.5,random_state=random_state)\n",
        "# Make sure images have shape (96, 96, 1) - Keras requirement!\n",
        "  x_train = np.expand_dims(x_train, -1)\n",
        "  x_test = np.expand_dims(x_test, -1)\n",
        "  x_valid = np.expand_dims(x_valid, -1)\n",
        "  x_nou=add_brightness(x_train,a,b)\n",
        "  x_nou_2=add_darkness(x_train,c,d)\n",
        "  x_gran = np.vstack((x_train,x_nou,x_nou_2))\n",
        "  y_gran = np.vstack((y_train,y_train,y_train))\n",
        "  return x_gran,y_gran,x_valid,y_valid\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7hXemtKY7FO"
      },
      "source": [
        "The *train()* function performs the model training.  As a new trick, we have changed the learning value (by default, it is 0.01). We have changed it to 0.001. We set the number of epochs to 80.\n",
        "\n",
        "The *prediction()* function gives us the mean pixel error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uPLAj9Dpdn8"
      },
      "source": [
        "def rmse_pixel(y_true, y_pred):\n",
        "        return np.sqrt(np.mean(np.square(y_pred*48 - y_true*48)))\n",
        "def train(x,y,batch_size=128,epochs=80,learning_rate=0.001):\n",
        "  optimizer = keras.optimizers.Adam(lr=0.01)\n",
        "  model.compile(loss=root_mean_squared_error, optimizer=optimizer)\n",
        "  K.set_value(model.optimizer.learning_rate, learning_rate)\n",
        "  model.fit(x, y, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "def prediction(x,y):\n",
        "  y_new = model.predict(x)\n",
        "  return rmse_pixel(y, y_new)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhIxy3tCZeXj"
      },
      "source": [
        "We perform an experiment with some different values for the random state number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iae1z3CVpdvk",
        "outputId": "b73ded2a-1878-451e-ad6d-bd4eb7f9fa84"
      },
      "source": [
        "rndm_st= [12,20,36,37,38,39,40,41,42,43,44]\n",
        "llista=[]\n",
        "for value in rndm_st:\n",
        "  x_gran,y_gran,x_valid,y_valid = dataset(value,1.5,0.4,-0.98,0.98)\n",
        "  # Predefined parameters\n",
        "  input_shape = (96, 96, 1)\n",
        "  output_shape = 30\n",
        "  model = keras.Sequential(\n",
        "      [\n",
        "          keras.Input(shape=input_shape),\n",
        "          layers.Conv2D(64, kernel_size=(3, 3),activation='relu', padding=\"same\"),     \n",
        "          layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "          layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"),\n",
        "          layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "          layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"),\n",
        "          layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "          layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"),\n",
        "          layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "          layers.Flatten(),\n",
        "          layers.Dropout(0.2),\n",
        "\n",
        "          layers.Dense(300,activation='relu'),\n",
        "          layers.Dense(300,activation='relu'),\n",
        "          layers.Dense(output_shape, activation=\"linear\")\n",
        "\n",
        "      ]\n",
        "  )\n",
        "  train(x_gran,y_gran)\n",
        "  llista.append(prediction(x_valid,y_valid))\n",
        "\n",
        "print('The mean value for all mean pixel errors is: '+str(np.mean(llista)))\n",
        "print('The minimum has been: '+str(np.min(llista)))\n",
        "print('The maximum has been: '+str(np.max(llista)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "32/32 [==============================] - 6s 85ms/step - loss: 0.1204 - val_loss: 0.0742\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0690 - val_loss: 0.0684\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0667 - val_loss: 0.0696\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0657 - val_loss: 0.0691\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0638 - val_loss: 0.0628\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0594 - val_loss: 0.0580\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0543 - val_loss: 0.0544\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0509 - val_loss: 0.0495\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0480 - val_loss: 0.0476\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 2s 58ms/step - loss: 0.0455 - val_loss: 0.0445\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0435 - val_loss: 0.0437\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0426 - val_loss: 0.0417\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0411 - val_loss: 0.0408\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0402 - val_loss: 0.0397\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0386 - val_loss: 0.0381\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0376 - val_loss: 0.0373\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0360 - val_loss: 0.0378\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0359 - val_loss: 0.0347\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0343 - val_loss: 0.0343\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0340 - val_loss: 0.0346\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0326 - val_loss: 0.0353\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0315 - val_loss: 0.0325\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0308 - val_loss: 0.0313\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0304 - val_loss: 0.0314\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0294 - val_loss: 0.0301\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0289 - val_loss: 0.0304\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0281 - val_loss: 0.0298\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0285 - val_loss: 0.0288\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0271 - val_loss: 0.0290\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0269 - val_loss: 0.0285\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0263 - val_loss: 0.0280\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0261 - val_loss: 0.0274\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0253 - val_loss: 0.0276\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0251 - val_loss: 0.0284\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0246 - val_loss: 0.0273\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0241 - val_loss: 0.0266\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0237 - val_loss: 0.0259\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0232 - val_loss: 0.0255\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0233 - val_loss: 0.0277\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0232 - val_loss: 0.0253\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0224 - val_loss: 0.0252\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0220 - val_loss: 0.0245\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0218 - val_loss: 0.0245\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0217 - val_loss: 0.0247\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0214 - val_loss: 0.0249\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0211 - val_loss: 0.0242\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0210 - val_loss: 0.0246\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0207 - val_loss: 0.0245\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0205 - val_loss: 0.0237\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0205 - val_loss: 0.0244\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0201 - val_loss: 0.0231\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0204 - val_loss: 0.0234\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0197 - val_loss: 0.0231\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0195 - val_loss: 0.0235\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0193 - val_loss: 0.0236\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0194 - val_loss: 0.0231\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0189 - val_loss: 0.0235\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0189 - val_loss: 0.0225\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0185 - val_loss: 0.0231\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0186 - val_loss: 0.0226\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0184 - val_loss: 0.0223\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0183 - val_loss: 0.0229\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0183 - val_loss: 0.0229\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0179 - val_loss: 0.0220\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0178 - val_loss: 0.0219\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0177 - val_loss: 0.0228\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0175 - val_loss: 0.0217\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0172 - val_loss: 0.0218\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0172 - val_loss: 0.0219\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0174 - val_loss: 0.0230\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0171 - val_loss: 0.0216\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0172 - val_loss: 0.0222\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0172 - val_loss: 0.0219\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0167 - val_loss: 0.0213\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0165 - val_loss: 0.0216\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0165 - val_loss: 0.0215\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0164 - val_loss: 0.0211\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0161 - val_loss: 0.0208\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0160 - val_loss: 0.0211\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0160 - val_loss: 0.0209\n",
            "Epoch 1/80\n",
            "32/32 [==============================] - 3s 65ms/step - loss: 0.1144 - val_loss: 0.0706\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0693 - val_loss: 0.0669\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0670 - val_loss: 0.0648\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0654 - val_loss: 0.0633\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0639 - val_loss: 0.0603\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0582 - val_loss: 0.0550\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0521 - val_loss: 0.0489\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0482 - val_loss: 0.0473\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0458 - val_loss: 0.0429\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0438 - val_loss: 0.0409\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0415 - val_loss: 0.0399\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0411 - val_loss: 0.0375\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0388 - val_loss: 0.0377\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0388 - val_loss: 0.0355\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0371 - val_loss: 0.0352\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0361 - val_loss: 0.0350\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0353 - val_loss: 0.0335\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0336 - val_loss: 0.0324\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0333 - val_loss: 0.0353\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0349 - val_loss: 0.0330\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0325 - val_loss: 0.0360\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0324 - val_loss: 0.0312\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0306 - val_loss: 0.0319\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0302 - val_loss: 0.0300\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0295 - val_loss: 0.0289\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0297 - val_loss: 0.0289\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0286 - val_loss: 0.0295\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0284 - val_loss: 0.0280\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0280 - val_loss: 0.0274\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0275 - val_loss: 0.0275\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0270 - val_loss: 0.0272\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0265 - val_loss: 0.0274\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0258 - val_loss: 0.0272\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0254 - val_loss: 0.0257\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0251 - val_loss: 0.0252\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0251 - val_loss: 0.0263\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0248 - val_loss: 0.0262\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0239 - val_loss: 0.0252\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0238 - val_loss: 0.0263\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0239 - val_loss: 0.0249\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 2s 66ms/step - loss: 0.0231 - val_loss: 0.0251\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 2s 65ms/step - loss: 0.0230 - val_loss: 0.0243\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0226 - val_loss: 0.0247\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0225 - val_loss: 0.0241\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0219 - val_loss: 0.0238\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0221 - val_loss: 0.0251\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0217 - val_loss: 0.0240\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0214 - val_loss: 0.0236\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0210 - val_loss: 0.0237\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0210 - val_loss: 0.0235\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0208 - val_loss: 0.0237\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0204 - val_loss: 0.0232\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0205 - val_loss: 0.0227\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0202 - val_loss: 0.0225\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0200 - val_loss: 0.0229\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0197 - val_loss: 0.0239\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0197 - val_loss: 0.0232\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0194 - val_loss: 0.0224\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0192 - val_loss: 0.0221\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0190 - val_loss: 0.0218\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0189 - val_loss: 0.0221\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0187 - val_loss: 0.0226\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0191 - val_loss: 0.0227\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0184 - val_loss: 0.0220\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0183 - val_loss: 0.0219\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0182 - val_loss: 0.0219\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0183 - val_loss: 0.0220\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0181 - val_loss: 0.0220\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0177 - val_loss: 0.0210\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0177 - val_loss: 0.0213\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0177 - val_loss: 0.0215\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0174 - val_loss: 0.0213\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0174 - val_loss: 0.0215\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0174 - val_loss: 0.0212\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0171 - val_loss: 0.0216\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0170 - val_loss: 0.0223\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0168 - val_loss: 0.0217\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0167 - val_loss: 0.0208\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0167 - val_loss: 0.0214\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0167 - val_loss: 0.0212\n",
            "Epoch 1/80\n",
            "32/32 [==============================] - 3s 64ms/step - loss: 0.1241 - val_loss: 0.0744\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0727 - val_loss: 0.0707\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0684 - val_loss: 0.0677\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0673 - val_loss: 0.0667\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0670 - val_loss: 0.0667\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0644 - val_loss: 0.0640\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0616 - val_loss: 0.0590\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0561 - val_loss: 0.0554\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0507 - val_loss: 0.0478\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0477 - val_loss: 0.0465\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0448 - val_loss: 0.0445\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0434 - val_loss: 0.0435\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0424 - val_loss: 0.0433\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0408 - val_loss: 0.0425\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0396 - val_loss: 0.0399\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0378 - val_loss: 0.0377\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0366 - val_loss: 0.0374\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0362 - val_loss: 0.0377\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0356 - val_loss: 0.0356\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0341 - val_loss: 0.0348\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0334 - val_loss: 0.0344\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0325 - val_loss: 0.0337\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0320 - val_loss: 0.0338\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0312 - val_loss: 0.0336\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0308 - val_loss: 0.0326\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0298 - val_loss: 0.0317\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0289 - val_loss: 0.0318\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0290 - val_loss: 0.0310\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0280 - val_loss: 0.0308\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0273 - val_loss: 0.0305\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0272 - val_loss: 0.0308\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0265 - val_loss: 0.0294\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0259 - val_loss: 0.0294\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0258 - val_loss: 0.0303\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0254 - val_loss: 0.0283\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0249 - val_loss: 0.0286\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0246 - val_loss: 0.0281\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0243 - val_loss: 0.0287\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 0.0240 - val_loss: 0.0282\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0232 - val_loss: 0.0274\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0232 - val_loss: 0.0276\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0228 - val_loss: 0.0287\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0228 - val_loss: 0.0280\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0223 - val_loss: 0.0275\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0224 - val_loss: 0.0271\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0222 - val_loss: 0.0267\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0216 - val_loss: 0.0265\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0211 - val_loss: 0.0270\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0209 - val_loss: 0.0264\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0206 - val_loss: 0.0257\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0205 - val_loss: 0.0265\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0204 - val_loss: 0.0264\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0203 - val_loss: 0.0259\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0199 - val_loss: 0.0255\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0197 - val_loss: 0.0262\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0197 - val_loss: 0.0255\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0194 - val_loss: 0.0261\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0190 - val_loss: 0.0252\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0190 - val_loss: 0.0252\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0192 - val_loss: 0.0251\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0190 - val_loss: 0.0247\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0185 - val_loss: 0.0247\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0184 - val_loss: 0.0252\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0186 - val_loss: 0.0244\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0181 - val_loss: 0.0242\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0179 - val_loss: 0.0240\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0177 - val_loss: 0.0248\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0178 - val_loss: 0.0246\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0177 - val_loss: 0.0248\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0173 - val_loss: 0.0243\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0172 - val_loss: 0.0241\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0172 - val_loss: 0.0234\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0173 - val_loss: 0.0239\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0170 - val_loss: 0.0237\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0170 - val_loss: 0.0243\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0167 - val_loss: 0.0237\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0165 - val_loss: 0.0235\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0164 - val_loss: 0.0239\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0163 - val_loss: 0.0234\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0163 - val_loss: 0.0244\n",
            "Epoch 1/80\n",
            "32/32 [==============================] - 3s 66ms/step - loss: 0.1225 - val_loss: 0.0719\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0717 - val_loss: 0.0698\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0681 - val_loss: 0.0662\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0664 - val_loss: 0.0643\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0649 - val_loss: 0.0630\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0629 - val_loss: 0.0608\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0586 - val_loss: 0.0557\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0541 - val_loss: 0.0506\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0495 - val_loss: 0.0482\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0477 - val_loss: 0.0453\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0445 - val_loss: 0.0428\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0429 - val_loss: 0.0419\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0417 - val_loss: 0.0402\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0401 - val_loss: 0.0395\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0402 - val_loss: 0.0398\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0379 - val_loss: 0.0374\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0374 - val_loss: 0.0370\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0364 - val_loss: 0.0370\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0359 - val_loss: 0.0356\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0339 - val_loss: 0.0349\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0334 - val_loss: 0.0331\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0323 - val_loss: 0.0334\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0318 - val_loss: 0.0328\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0316 - val_loss: 0.0353\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0307 - val_loss: 0.0324\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0303 - val_loss: 0.0314\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0294 - val_loss: 0.0312\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0286 - val_loss: 0.0303\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0283 - val_loss: 0.0314\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0283 - val_loss: 0.0302\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0271 - val_loss: 0.0297\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0276 - val_loss: 0.0315\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0266 - val_loss: 0.0295\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0259 - val_loss: 0.0285\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0254 - val_loss: 0.0290\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0252 - val_loss: 0.0283\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0245 - val_loss: 0.0293\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0244 - val_loss: 0.0298\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0242 - val_loss: 0.0283\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0242 - val_loss: 0.0278\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0233 - val_loss: 0.0277\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0234 - val_loss: 0.0280\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0228 - val_loss: 0.0272\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0223 - val_loss: 0.0274\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0225 - val_loss: 0.0273\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0221 - val_loss: 0.0280\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0223 - val_loss: 0.0271\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0219 - val_loss: 0.0274\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0214 - val_loss: 0.0273\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0211 - val_loss: 0.0271\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0211 - val_loss: 0.0275\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0210 - val_loss: 0.0270\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0204 - val_loss: 0.0267\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0200 - val_loss: 0.0263\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0201 - val_loss: 0.0268\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0199 - val_loss: 0.0264\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0198 - val_loss: 0.0276\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0196 - val_loss: 0.0265\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0193 - val_loss: 0.0259\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0190 - val_loss: 0.0257\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0192 - val_loss: 0.0264\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0188 - val_loss: 0.0259\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0188 - val_loss: 0.0257\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0185 - val_loss: 0.0260\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0183 - val_loss: 0.0256\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0184 - val_loss: 0.0260\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0181 - val_loss: 0.0254\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0179 - val_loss: 0.0252\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0181 - val_loss: 0.0254\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0179 - val_loss: 0.0256\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0177 - val_loss: 0.0250\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0175 - val_loss: 0.0252\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0174 - val_loss: 0.0253\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0173 - val_loss: 0.0256\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0173 - val_loss: 0.0252\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0172 - val_loss: 0.0246\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0172 - val_loss: 0.0254\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0170 - val_loss: 0.0250\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0167 - val_loss: 0.0255\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0168 - val_loss: 0.0250\n",
            "Epoch 1/80\n",
            "32/32 [==============================] - 3s 65ms/step - loss: 0.1199 - val_loss: 0.0758\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0711 - val_loss: 0.0700\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0680 - val_loss: 0.0664\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0668 - val_loss: 0.0674\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0651 - val_loss: 0.0635\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0620 - val_loss: 0.0598\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0580 - val_loss: 0.0532\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0551 - val_loss: 0.0497\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0511 - val_loss: 0.0485\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0491 - val_loss: 0.0480\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0470 - val_loss: 0.0449\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0454 - val_loss: 0.0427\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0444 - val_loss: 0.0428\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0427 - val_loss: 0.0433\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0415 - val_loss: 0.0411\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0397 - val_loss: 0.0410\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0388 - val_loss: 0.0394\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0383 - val_loss: 0.0370\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0371 - val_loss: 0.0360\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0356 - val_loss: 0.0354\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0346 - val_loss: 0.0346\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0347 - val_loss: 0.0354\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0336 - val_loss: 0.0335\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0332 - val_loss: 0.0341\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0318 - val_loss: 0.0355\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0314 - val_loss: 0.0319\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0304 - val_loss: 0.0327\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0301 - val_loss: 0.0334\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0299 - val_loss: 0.0317\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0286 - val_loss: 0.0310\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0282 - val_loss: 0.0300\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0278 - val_loss: 0.0304\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0275 - val_loss: 0.0297\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0272 - val_loss: 0.0295\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0266 - val_loss: 0.0301\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0263 - val_loss: 0.0299\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0264 - val_loss: 0.0299\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0255 - val_loss: 0.0291\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0258 - val_loss: 0.0292\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0253 - val_loss: 0.0284\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0247 - val_loss: 0.0280\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0241 - val_loss: 0.0292\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0238 - val_loss: 0.0275\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0234 - val_loss: 0.0270\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0231 - val_loss: 0.0272\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0226 - val_loss: 0.0270\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0224 - val_loss: 0.0267\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0223 - val_loss: 0.0269\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0219 - val_loss: 0.0260\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0217 - val_loss: 0.0260\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0213 - val_loss: 0.0269\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0210 - val_loss: 0.0253\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0208 - val_loss: 0.0256\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0205 - val_loss: 0.0253\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0206 - val_loss: 0.0261\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0206 - val_loss: 0.0254\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0204 - val_loss: 0.0252\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0199 - val_loss: 0.0248\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0198 - val_loss: 0.0250\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0197 - val_loss: 0.0250\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0195 - val_loss: 0.0249\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0193 - val_loss: 0.0245\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0190 - val_loss: 0.0244\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0188 - val_loss: 0.0245\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0189 - val_loss: 0.0246\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0192 - val_loss: 0.0243\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0187 - val_loss: 0.0244\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0184 - val_loss: 0.0240\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0183 - val_loss: 0.0244\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0179 - val_loss: 0.0241\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0179 - val_loss: 0.0238\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0176 - val_loss: 0.0237\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0176 - val_loss: 0.0238\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0181 - val_loss: 0.0239\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0174 - val_loss: 0.0237\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0171 - val_loss: 0.0234\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0171 - val_loss: 0.0231\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0168 - val_loss: 0.0239\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0171 - val_loss: 0.0238\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0168 - val_loss: 0.0229\n",
            "Epoch 1/80\n",
            "32/32 [==============================] - 3s 68ms/step - loss: 0.1244 - val_loss: 0.0739\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0714 - val_loss: 0.0660\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0674 - val_loss: 0.0643\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0663 - val_loss: 0.0627\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0640 - val_loss: 0.0590\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0595 - val_loss: 0.0600\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0571 - val_loss: 0.0554\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0511 - val_loss: 0.0487\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0486 - val_loss: 0.0439\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0463 - val_loss: 0.0447\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0440 - val_loss: 0.0414\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0421 - val_loss: 0.0388\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0416 - val_loss: 0.0422\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0395 - val_loss: 0.0370\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0379 - val_loss: 0.0355\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0373 - val_loss: 0.0359\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0369 - val_loss: 0.0354\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0357 - val_loss: 0.0336\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0346 - val_loss: 0.0342\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0339 - val_loss: 0.0328\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0339 - val_loss: 0.0322\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0325 - val_loss: 0.0317\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0316 - val_loss: 0.0305\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0313 - val_loss: 0.0301\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0313 - val_loss: 0.0303\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0302 - val_loss: 0.0294\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0298 - val_loss: 0.0297\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0294 - val_loss: 0.0290\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0282 - val_loss: 0.0283\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0283 - val_loss: 0.0282\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0277 - val_loss: 0.0287\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0276 - val_loss: 0.0277\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0266 - val_loss: 0.0274\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0260 - val_loss: 0.0269\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0262 - val_loss: 0.0268\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0255 - val_loss: 0.0263\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0251 - val_loss: 0.0266\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0246 - val_loss: 0.0260\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0250 - val_loss: 0.0268\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0247 - val_loss: 0.0260\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0235 - val_loss: 0.0254\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0235 - val_loss: 0.0255\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0234 - val_loss: 0.0254\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0231 - val_loss: 0.0250\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0232 - val_loss: 0.0263\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0224 - val_loss: 0.0256\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0223 - val_loss: 0.0260\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0219 - val_loss: 0.0246\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0217 - val_loss: 0.0252\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0216 - val_loss: 0.0252\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0211 - val_loss: 0.0241\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0212 - val_loss: 0.0247\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0208 - val_loss: 0.0237\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0205 - val_loss: 0.0239\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0204 - val_loss: 0.0236\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0199 - val_loss: 0.0237\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0201 - val_loss: 0.0240\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0202 - val_loss: 0.0240\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0195 - val_loss: 0.0236\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0197 - val_loss: 0.0244\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0194 - val_loss: 0.0239\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0190 - val_loss: 0.0233\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0189 - val_loss: 0.0234\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0188 - val_loss: 0.0235\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0186 - val_loss: 0.0235\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0187 - val_loss: 0.0229\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0185 - val_loss: 0.0230\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0181 - val_loss: 0.0234\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0181 - val_loss: 0.0230\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0181 - val_loss: 0.0230\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0182 - val_loss: 0.0228\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0181 - val_loss: 0.0230\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0179 - val_loss: 0.0228\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0183 - val_loss: 0.0241\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0175 - val_loss: 0.0227\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0172 - val_loss: 0.0229\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0171 - val_loss: 0.0230\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0171 - val_loss: 0.0226\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0170 - val_loss: 0.0227\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0169 - val_loss: 0.0230\n",
            "Epoch 1/80\n",
            "32/32 [==============================] - 3s 67ms/step - loss: 0.1320 - val_loss: 0.0708\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0702 - val_loss: 0.0665\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0685 - val_loss: 0.0670\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0669 - val_loss: 0.0634\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0654 - val_loss: 0.0654\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0637 - val_loss: 0.0588\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0590 - val_loss: 0.0534\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0535 - val_loss: 0.0506\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0507 - val_loss: 0.0471\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0480 - val_loss: 0.0444\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0461 - val_loss: 0.0432\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0446 - val_loss: 0.0428\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0432 - val_loss: 0.0403\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0412 - val_loss: 0.0407\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0404 - val_loss: 0.0383\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0392 - val_loss: 0.0373\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0383 - val_loss: 0.0382\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0378 - val_loss: 0.0375\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0361 - val_loss: 0.0352\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0358 - val_loss: 0.0353\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0346 - val_loss: 0.0349\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0334 - val_loss: 0.0329\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0325 - val_loss: 0.0331\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0324 - val_loss: 0.0323\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0318 - val_loss: 0.0318\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0304 - val_loss: 0.0312\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0305 - val_loss: 0.0307\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0301 - val_loss: 0.0320\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0291 - val_loss: 0.0309\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0284 - val_loss: 0.0297\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0283 - val_loss: 0.0294\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0285 - val_loss: 0.0304\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0271 - val_loss: 0.0287\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0266 - val_loss: 0.0293\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0263 - val_loss: 0.0282\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0262 - val_loss: 0.0278\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0257 - val_loss: 0.0273\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0247 - val_loss: 0.0274\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0246 - val_loss: 0.0275\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0245 - val_loss: 0.0297\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0248 - val_loss: 0.0266\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0237 - val_loss: 0.0262\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0233 - val_loss: 0.0269\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0228 - val_loss: 0.0270\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0228 - val_loss: 0.0260\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0227 - val_loss: 0.0265\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0224 - val_loss: 0.0255\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0221 - val_loss: 0.0257\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0218 - val_loss: 0.0253\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0216 - val_loss: 0.0253\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0212 - val_loss: 0.0254\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0210 - val_loss: 0.0250\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0209 - val_loss: 0.0250\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0205 - val_loss: 0.0246\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0206 - val_loss: 0.0246\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0208 - val_loss: 0.0248\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0202 - val_loss: 0.0243\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0199 - val_loss: 0.0243\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0196 - val_loss: 0.0242\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0194 - val_loss: 0.0239\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0195 - val_loss: 0.0241\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0192 - val_loss: 0.0239\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0192 - val_loss: 0.0244\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0193 - val_loss: 0.0241\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0190 - val_loss: 0.0233\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0186 - val_loss: 0.0233\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0183 - val_loss: 0.0234\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0185 - val_loss: 0.0244\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0182 - val_loss: 0.0234\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0181 - val_loss: 0.0238\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0180 - val_loss: 0.0238\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0180 - val_loss: 0.0236\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0177 - val_loss: 0.0238\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0178 - val_loss: 0.0236\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0176 - val_loss: 0.0227\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0174 - val_loss: 0.0228\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0173 - val_loss: 0.0234\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0172 - val_loss: 0.0227\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0172 - val_loss: 0.0226\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0169 - val_loss: 0.0231\n",
            "Epoch 1/80\n",
            "32/32 [==============================] - 3s 64ms/step - loss: 0.1473 - val_loss: 0.0817\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0716 - val_loss: 0.0704\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0684 - val_loss: 0.0701\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0675 - val_loss: 0.0664\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0652 - val_loss: 0.0641\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0649 - val_loss: 0.0665\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0591 - val_loss: 0.0559\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0531 - val_loss: 0.0523\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0509 - val_loss: 0.0486\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0486 - val_loss: 0.0486\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0455 - val_loss: 0.0430\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0429 - val_loss: 0.0423\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0419 - val_loss: 0.0392\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0406 - val_loss: 0.0446\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0403 - val_loss: 0.0391\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0383 - val_loss: 0.0366\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0379 - val_loss: 0.0365\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0363 - val_loss: 0.0372\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0357 - val_loss: 0.0351\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0351 - val_loss: 0.0364\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0344 - val_loss: 0.0333\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0341 - val_loss: 0.0343\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0337 - val_loss: 0.0338\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0329 - val_loss: 0.0314\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0316 - val_loss: 0.0308\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0312 - val_loss: 0.0311\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0305 - val_loss: 0.0310\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0300 - val_loss: 0.0298\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0294 - val_loss: 0.0302\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0290 - val_loss: 0.0292\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0286 - val_loss: 0.0298\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0284 - val_loss: 0.0304\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0280 - val_loss: 0.0295\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0273 - val_loss: 0.0283\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0271 - val_loss: 0.0293\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0265 - val_loss: 0.0278\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0260 - val_loss: 0.0277\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0254 - val_loss: 0.0274\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0251 - val_loss: 0.0273\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0251 - val_loss: 0.0297\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0253 - val_loss: 0.0275\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0247 - val_loss: 0.0268\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0238 - val_loss: 0.0269\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0238 - val_loss: 0.0265\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0234 - val_loss: 0.0269\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0234 - val_loss: 0.0272\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0229 - val_loss: 0.0262\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0226 - val_loss: 0.0252\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0224 - val_loss: 0.0259\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0223 - val_loss: 0.0255\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0218 - val_loss: 0.0254\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0217 - val_loss: 0.0260\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0219 - val_loss: 0.0248\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0211 - val_loss: 0.0249\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0206 - val_loss: 0.0251\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0205 - val_loss: 0.0244\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0204 - val_loss: 0.0245\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0203 - val_loss: 0.0250\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0203 - val_loss: 0.0248\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0203 - val_loss: 0.0257\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0198 - val_loss: 0.0245\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0195 - val_loss: 0.0242\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0196 - val_loss: 0.0236\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0195 - val_loss: 0.0238\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0190 - val_loss: 0.0239\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0189 - val_loss: 0.0243\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0188 - val_loss: 0.0237\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0185 - val_loss: 0.0242\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0185 - val_loss: 0.0235\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0185 - val_loss: 0.0234\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0181 - val_loss: 0.0234\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0181 - val_loss: 0.0234\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0180 - val_loss: 0.0229\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0178 - val_loss: 0.0236\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0176 - val_loss: 0.0236\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0174 - val_loss: 0.0231\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0178 - val_loss: 0.0231\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0173 - val_loss: 0.0230\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0171 - val_loss: 0.0228\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0171 - val_loss: 0.0229\n",
            "Epoch 1/80\n",
            "32/32 [==============================] - 3s 66ms/step - loss: 0.1363 - val_loss: 0.0743\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0714 - val_loss: 0.0685\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0670 - val_loss: 0.0665\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0651 - val_loss: 0.0646\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0620 - val_loss: 0.0589\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0562 - val_loss: 0.0513\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0512 - val_loss: 0.0506\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0486 - val_loss: 0.0560\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0477 - val_loss: 0.0447\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0439 - val_loss: 0.0432\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0425 - val_loss: 0.0410\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0417 - val_loss: 0.0386\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0397 - val_loss: 0.0374\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 0.0379 - val_loss: 0.0385\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0376 - val_loss: 0.0370\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0364 - val_loss: 0.0353\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 0.0355 - val_loss: 0.0345\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0347 - val_loss: 0.0357\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0341 - val_loss: 0.0339\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0328 - val_loss: 0.0325\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0323 - val_loss: 0.0320\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0313 - val_loss: 0.0319\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0314 - val_loss: 0.0321\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0300 - val_loss: 0.0315\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0294 - val_loss: 0.0299\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0285 - val_loss: 0.0285\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0282 - val_loss: 0.0288\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0275 - val_loss: 0.0297\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0271 - val_loss: 0.0279\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0279 - val_loss: 0.0284\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0265 - val_loss: 0.0283\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0260 - val_loss: 0.0273\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0259 - val_loss: 0.0279\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0255 - val_loss: 0.0283\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0252 - val_loss: 0.0269\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0247 - val_loss: 0.0275\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0240 - val_loss: 0.0260\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0239 - val_loss: 0.0268\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0235 - val_loss: 0.0260\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0233 - val_loss: 0.0258\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0233 - val_loss: 0.0251\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0228 - val_loss: 0.0250\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0227 - val_loss: 0.0253\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0225 - val_loss: 0.0249\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0219 - val_loss: 0.0247\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0221 - val_loss: 0.0255\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0215 - val_loss: 0.0250\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0211 - val_loss: 0.0251\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0214 - val_loss: 0.0245\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0208 - val_loss: 0.0247\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0207 - val_loss: 0.0244\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0206 - val_loss: 0.0246\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0201 - val_loss: 0.0244\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0202 - val_loss: 0.0236\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0199 - val_loss: 0.0238\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0196 - val_loss: 0.0242\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0197 - val_loss: 0.0242\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0194 - val_loss: 0.0241\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0190 - val_loss: 0.0232\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0188 - val_loss: 0.0237\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0187 - val_loss: 0.0238\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0187 - val_loss: 0.0232\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0185 - val_loss: 0.0231\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0187 - val_loss: 0.0231\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0184 - val_loss: 0.0233\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0180 - val_loss: 0.0242\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0178 - val_loss: 0.0228\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0179 - val_loss: 0.0237\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0180 - val_loss: 0.0238\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0176 - val_loss: 0.0233\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0177 - val_loss: 0.0240\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0174 - val_loss: 0.0228\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0171 - val_loss: 0.0228\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0170 - val_loss: 0.0223\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0171 - val_loss: 0.0233\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0172 - val_loss: 0.0225\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0169 - val_loss: 0.0229\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0169 - val_loss: 0.0230\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0167 - val_loss: 0.0227\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0166 - val_loss: 0.0222\n",
            "Epoch 1/80\n",
            "32/32 [==============================] - 3s 65ms/step - loss: 0.1402 - val_loss: 0.0787\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0739 - val_loss: 0.0703\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0682 - val_loss: 0.0679\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0676 - val_loss: 0.0667\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0659 - val_loss: 0.0664\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0646 - val_loss: 0.0643\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0622 - val_loss: 0.0609\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0585 - val_loss: 0.0556\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0545 - val_loss: 0.0543\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0506 - val_loss: 0.0485\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0485 - val_loss: 0.0455\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0460 - val_loss: 0.0442\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0446 - val_loss: 0.0434\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0432 - val_loss: 0.0425\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0417 - val_loss: 0.0434\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0408 - val_loss: 0.0407\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0389 - val_loss: 0.0384\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0389 - val_loss: 0.0397\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0373 - val_loss: 0.0382\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0363 - val_loss: 0.0376\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0355 - val_loss: 0.0356\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0343 - val_loss: 0.0345\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0341 - val_loss: 0.0341\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0332 - val_loss: 0.0338\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0320 - val_loss: 0.0328\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0318 - val_loss: 0.0339\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0314 - val_loss: 0.0332\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0303 - val_loss: 0.0313\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0296 - val_loss: 0.0308\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0288 - val_loss: 0.0304\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0285 - val_loss: 0.0313\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0278 - val_loss: 0.0299\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0277 - val_loss: 0.0306\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0273 - val_loss: 0.0301\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0273 - val_loss: 0.0290\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0259 - val_loss: 0.0285\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0259 - val_loss: 0.0281\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0250 - val_loss: 0.0286\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0251 - val_loss: 0.0280\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0242 - val_loss: 0.0272\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0240 - val_loss: 0.0275\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0234 - val_loss: 0.0268\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0231 - val_loss: 0.0269\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0231 - val_loss: 0.0265\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0231 - val_loss: 0.0274\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 2s 59ms/step - loss: 0.0227 - val_loss: 0.0271\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0223 - val_loss: 0.0268\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0222 - val_loss: 0.0264\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0218 - val_loss: 0.0256\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0214 - val_loss: 0.0263\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0213 - val_loss: 0.0263\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0210 - val_loss: 0.0255\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0208 - val_loss: 0.0250\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0205 - val_loss: 0.0252\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0204 - val_loss: 0.0257\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0203 - val_loss: 0.0248\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0199 - val_loss: 0.0252\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0198 - val_loss: 0.0252\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0195 - val_loss: 0.0242\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0194 - val_loss: 0.0243\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0193 - val_loss: 0.0243\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0191 - val_loss: 0.0246\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0190 - val_loss: 0.0240\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0188 - val_loss: 0.0246\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 0.0188 - val_loss: 0.0241\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0187 - val_loss: 0.0240\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0184 - val_loss: 0.0235\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0184 - val_loss: 0.0233\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0182 - val_loss: 0.0238\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0181 - val_loss: 0.0238\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 0.0177 - val_loss: 0.0234\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0175 - val_loss: 0.0241\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0177 - val_loss: 0.0239\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0174 - val_loss: 0.0231\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0173 - val_loss: 0.0240\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0170 - val_loss: 0.0234\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0171 - val_loss: 0.0227\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0168 - val_loss: 0.0226\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0169 - val_loss: 0.0233\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0169 - val_loss: 0.0226\n",
            "Epoch 1/80\n",
            "32/32 [==============================] - 3s 65ms/step - loss: 0.1091 - val_loss: 0.0694\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0688 - val_loss: 0.0666\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0674 - val_loss: 0.0682\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0668 - val_loss: 0.0646\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0652 - val_loss: 0.0681\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0635 - val_loss: 0.0609\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0596 - val_loss: 0.0611\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0554 - val_loss: 0.0542\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0506 - val_loss: 0.0486\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0473 - val_loss: 0.0442\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0448 - val_loss: 0.0441\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0429 - val_loss: 0.0422\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0416 - val_loss: 0.0399\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0399 - val_loss: 0.0397\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0392 - val_loss: 0.0371\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0376 - val_loss: 0.0382\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0366 - val_loss: 0.0353\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 0.0352 - val_loss: 0.0346\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0347 - val_loss: 0.0339\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0347 - val_loss: 0.0340\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0329 - val_loss: 0.0328\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0320 - val_loss: 0.0316\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0314 - val_loss: 0.0314\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0310 - val_loss: 0.0314\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0310 - val_loss: 0.0305\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0299 - val_loss: 0.0301\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0290 - val_loss: 0.0294\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0293 - val_loss: 0.0295\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0281 - val_loss: 0.0285\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0276 - val_loss: 0.0282\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0274 - val_loss: 0.0292\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0266 - val_loss: 0.0278\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0267 - val_loss: 0.0280\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0264 - val_loss: 0.0270\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0256 - val_loss: 0.0270\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0253 - val_loss: 0.0268\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0249 - val_loss: 0.0266\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0244 - val_loss: 0.0262\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0242 - val_loss: 0.0260\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0237 - val_loss: 0.0260\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0234 - val_loss: 0.0251\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 0.0232 - val_loss: 0.0255\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0229 - val_loss: 0.0254\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0232 - val_loss: 0.0248\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0225 - val_loss: 0.0263\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0222 - val_loss: 0.0247\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0217 - val_loss: 0.0247\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0215 - val_loss: 0.0245\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0214 - val_loss: 0.0242\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 2s 65ms/step - loss: 0.0210 - val_loss: 0.0239\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0207 - val_loss: 0.0240\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 0.0207 - val_loss: 0.0238\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0203 - val_loss: 0.0235\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0200 - val_loss: 0.0235\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0201 - val_loss: 0.0232\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0196 - val_loss: 0.0236\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0194 - val_loss: 0.0234\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0194 - val_loss: 0.0243\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0193 - val_loss: 0.0231\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 0.0190 - val_loss: 0.0226\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0186 - val_loss: 0.0227\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 2s 64ms/step - loss: 0.0187 - val_loss: 0.0236\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0190 - val_loss: 0.0229\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0186 - val_loss: 0.0224\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0185 - val_loss: 0.0226\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0180 - val_loss: 0.0224\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0181 - val_loss: 0.0220\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0176 - val_loss: 0.0223\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0177 - val_loss: 0.0222\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0178 - val_loss: 0.0218\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0177 - val_loss: 0.0222\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0174 - val_loss: 0.0223\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 2s 60ms/step - loss: 0.0172 - val_loss: 0.0221\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0170 - val_loss: 0.0219\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0170 - val_loss: 0.0219\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0170 - val_loss: 0.0212\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0168 - val_loss: 0.0216\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 2s 63ms/step - loss: 0.0165 - val_loss: 0.0218\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 2s 61ms/step - loss: 0.0165 - val_loss: 0.0215\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0163 - val_loss: 0.0213\n",
            "The mean value for all mean pixel errors is: 1.3950094\n",
            "The minimum has been: 1.3129808\n",
            "The maximum has been: 1.4630226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KdcaTi8Zx9g"
      },
      "source": [
        "As we can see, last time I compiled the code, I obtained: \n",
        "- A mean value for all mean pixel errors of $1.3950094$\n",
        "- The best result that I got in this iteration was a mean pixel error of $1.3129808$\n",
        "- The worst result that I got in this iteration was a mean pixel error of $1.4630226$\n"
      ]
    }
  ]
}